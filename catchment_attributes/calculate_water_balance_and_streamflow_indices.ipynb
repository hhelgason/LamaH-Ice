{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "prescription-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ready-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook reads streamflow measurements and meteorological data\n",
    "# and exports a dataframe with water balance statistics and streamflow indices.\n",
    "# Winter streamflow data is missing for some years. Only years with >90% temporal data coverage are considered.\n",
    "# Gauges with less than 3 years of valid data are omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "foster-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\code\\HydroAnalysis')\n",
    "import hydroanalysis\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import geopandas as gpds\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "pds.set_option('display.max_rows', 500)\n",
    "from pathlib import Path\n",
    "\n",
    "# Define a function to read csv files containing ERA5-Land data\n",
    "def read_era_csv(path,start,end):\n",
    "    var = pds.read_csv(path)\n",
    "    var = var.set_index('YYYY MM DD')\n",
    "    var.index = pds.to_datetime(var.index)\n",
    "    var = var[start:end]\n",
    "    var.columns = var.columns.astype(int)\n",
    "    var.sort_index(axis=1,inplace=True)\n",
    "    return(var)\n",
    "\n",
    "def combine_csv_files(csv_directory):\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    from pathlib import Path\n",
    "\n",
    "    # List the CSV files\n",
    "    csv_files = glob.glob(str(csv_directory / '*.csv'))\n",
    "\n",
    "    # Initialize an empty DataFrame for combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Extract catchment ID from filename\n",
    "        catchment_id = Path(file).stem\n",
    "\n",
    "        # Rename the 'PET' column to the catchment ID\n",
    "        df = df.rename(columns={'PET': catchment_id})\n",
    "\n",
    "        # Merge with the combined DataFrame\n",
    "        if combined_df.empty:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = pd.merge(combined_df, df, on='date', how='outer')\n",
    "\n",
    "    # The combined_df now contains all the data with catchment IDs as column headers\n",
    "    combined_df = combined_df.set_index('date')\n",
    "    combined_df.index = pds.to_datetime(combined_df.index)\n",
    "    return(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-mission",
   "metadata": {},
   "source": [
    "# Read meteorological data and measured streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "constitutional-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read precip, evap and PET from ERA5-Land\n",
    "start = '1981-10-01'\n",
    "end = '2018-09-30'\n",
    "path_precip = Path(r'C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\era5_land\\1950-2021\\daily\\ERA5L_total_precipitation.csv')\n",
    "path_total_evap = Path(r'C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\era5_land\\1950-2021\\daily\\ERA5L_total_evaporation.csv')\n",
    "path_potential_evap = Path(r'C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\era5_land\\1950-2021\\daily\\ERA5L_potential_evaporation.csv')\n",
    "\n",
    "precip = read_era_csv(path_precip,start,end)\n",
    "evap = read_era_csv(path_total_evap,start,end)\n",
    "pet = read_era_csv(path_potential_evap,start,end)\n",
    "\n",
    "# Read precip from RAV-II\n",
    "rav_data = pds.read_csv(Path(r\"C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\rav2\\rav2_precip_daily_Basins_A.csv\")) #header=TRUE,\n",
    "rav_data.index=pds.to_datetime(rav_data['Time'])\n",
    "rav_data = rav_data.drop(columns=['Time'])\n",
    "rav_data = rav_data[start:end]\n",
    "rav_data.columns = rav_data.columns.astype(int)\n",
    "rav_data.sort_index(axis=1,inplace=True)\n",
    "\n",
    "# Read ET from RAV-II\n",
    "rav_ET_data = pds.read_csv(Path(r\"C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\rav2\\rav2_ET_hourly_Basins_A.csv\")) #header=TRUE,\n",
    "rav_ET_data.index=pds.to_datetime(rav_ET_data['Time'])\n",
    "rav_ET_data = rav_ET_data.drop(columns=['Time'])\n",
    "# Calc daily sums\n",
    "rav_ET_sum = rav_ET_data.resample('D').sum()\n",
    "rav_ET_sum = rav_ET_sum[start:end]\n",
    "rav_ET_sum.columns = rav_ET_sum.columns.astype(int)\n",
    "rav_ET_sum.sort_index(axis=1,inplace=True)\n",
    "\n",
    "# Read PET from RAV-II\n",
    "csv_directory = Path(r\"C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\PET_Calculations\\ref_ET_from_RAV_PM\")\n",
    "PET_PM_ravII_grdflx = combine_csv_files(csv_directory)\n",
    "PET_PM_ravII_grdflx = PET_PM_ravII_grdflx[start:end]\n",
    "PET_PM_ravII_grdflx.columns = pds.to_numeric(PET_PM_ravII_grdflx.columns)\n",
    "PET_PM_ravII_grdflx = PET_PM_ravII_grdflx.sort_index(axis=1)\n",
    "PET_PM_ravII_grdflx = PET_PM_ravII_grdflx[PET_PM_ravII_grdflx.columns[:-3]]\n",
    "PET_PM_ravII_grdflx.columns = PET_PM_ravII_grdflx.columns.astype(int)\n",
    "PET_PM_ravII_grdflx.sort_index(axis=1,inplace=True)\n",
    "\n",
    "# Read precip data from CARRA:\n",
    "combined_carra = pds.read_csv(Path(r\"C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\carra\\precip_daily\\carra_precip_daily_Basins_A.csv\")) #header=TRUE,\n",
    "combined_carra.index=pds.to_datetime(combined_carra['time'])\n",
    "combined_carra = combined_carra.drop(columns=['time'])\n",
    "combined_carra.columns = combined_carra.columns.astype(int)\n",
    "combined_carra.sort_index(axis=1,inplace=True)\n",
    "\n",
    "# Read the measured streamflow:\n",
    "save_date = 'dec6_2023'\n",
    "savepath = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\discharge_measurements\\processed_by_hh\\combined_gauges_LV_VI_raw_%s.p\" % save_date)\n",
    "combined_dict_npc_met_office = pickle.load(open( savepath, \"rb\" ) )\n",
    "savepath = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\discharge_measurements\\processed_by_hh\\combined_gauges_LV_VI_raw_splitted_%s.p\" % save_date)\n",
    "splitted_gauge_dict = pickle.load(open( savepath, \"rb\" ) )\n",
    "savepath = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\discharge_measurements\\processed_by_hh\\combined_gauges_LV_VI_highqual_%s.p\" % save_date)\n",
    "combined_dict_high_qual = pickle.load(open( savepath, \"rb\" ) )\n",
    "savepath = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\discharge_measurements\\processed_by_hh\\combined_gauges_LV_VI_highqual_splitted_%s.p\" % save_date)\n",
    "splitted_gauge_dict_high_qual = pickle.load(open( savepath, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cb633",
   "metadata": {},
   "source": [
    "# Read the gauges shapefile and the combined attributes file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0950f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read gauges shapefile\n",
    "gauges = gpds.read_file(r'C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\lamah_ice\\D_gauges\\3_shapefiles\\gauges.shp')\n",
    "gauges.index = gauges['id'].astype(int)\n",
    "gauges.index.name = 'id'\n",
    "gauges.sort_index(inplace=True)\n",
    "\n",
    "catchment_attrs_path = Path(r\"C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\lamah_ice\\A_basins_total_upstrm\\1_attributes\\Catchment_attributes.csv\")\n",
    "catchment_attrs = pds.read_csv(catchment_attrs_path,sep=';')\n",
    "catchment_attrs.set_index('id',inplace=True)\n",
    "catchment_attrs['lat'] = gauges.geometry.y\n",
    "catchment_attrs['lon'] = gauges.geometry.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-briefs",
   "metadata": {},
   "source": [
    "# Export a water balance .csv file and calculate signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fourth-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider two versions of the streamflow observations: Unfiltered data (all measurements) and filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-emergency",
   "metadata": {},
   "source": [
    "# Filtered observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "color-latex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean valid years: year_count    15.027397\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with measurements that are ready for processing by the hydroanalysis package:\n",
    "meas_dict = dict()\n",
    "for gauge in splitted_gauge_dict_high_qual.keys():\n",
    "    \n",
    "    df = splitted_gauge_dict_high_qual[gauge].copy()\n",
    "    df.columns=['Value','Quality']\n",
    "    \n",
    "    # For the HydroAnalysis package, the quality needs to be on the format 0 (good) or 1 (not used)\n",
    "    df.loc[df['Quality'] <= 100, 'Quality'] = 0\n",
    "    # For filtered data, the only instances where the quality code is > 100 is when data is missing (QC=250).\n",
    "    # Either way, we set the 'Quality' stamp to 1\n",
    "    df.loc[df['Quality'] > 100, 'Quality'] = 1\n",
    "    \n",
    "    # Rename the splitted gauge names (V112_1, V100_1) to their proper gauge names (V112,V100)\n",
    "    if gauge=='V112_1':\n",
    "        gauge='V112'\n",
    "    elif gauge=='V68_1':\n",
    "        gauge='V68'\n",
    "    elif gauge=='V100_1':\n",
    "        gauge='V100'\n",
    "    meas_dict[gauge] = df\n",
    "\n",
    "# Create a dictionary that contains a dataframe for each gauge\n",
    "# The dataframe has the columns: \n",
    "# Streamflow meas, quality code, precip from ERA5-Land and RAV-II, PET, total evaporation from ERA5-Land and water year\n",
    "# If we have less than 3 years of valid data, we don't include the gauge\n",
    "\n",
    "data_for_valid_years = dict()\n",
    "\n",
    "# This dictionary keeps track of how many valid years we have for each gauge\n",
    "valid_years_lengths = dict()\n",
    "\n",
    "thresh = 0.9\n",
    "\n",
    "for gauge in meas_dict.keys():\n",
    "    # Fetch the gauge ID from the station name\n",
    "    gauge_id = gauges[gauges['V_no']==gauge].index[0] \n",
    "\n",
    "    # Load the discharge values\n",
    "    df = meas_dict[gauge].copy()\n",
    "\n",
    "    try:\n",
    "        df.index= df.index.date\n",
    "    except:\n",
    "        print('df index is already on date format for %s' %gauge)\n",
    "\n",
    "    # Load ERA precip\n",
    "    wshed_precip = precip[gauge_id]\n",
    "    precip_df = pds.DataFrame(wshed_precip)\n",
    "    precip_df.columns=['P_ERA5L']\n",
    "    df['P_ERA5L'] = precip_df\n",
    "    \n",
    "    # Load PET\n",
    "    wshed_pet = pet[gauge_id]\n",
    "    pet_df = pds.DataFrame(wshed_pet)\n",
    "    pet_df.columns=['PET_ERA5L']\n",
    "    df['PET_ERA5L'] = pet_df\n",
    "    \n",
    "    # Load total evaporation\n",
    "    wshed_evap = evap[gauge_id]\n",
    "    et_df = pds.DataFrame(wshed_evap)\n",
    "    et_df.columns=['ET_ERA5L']\n",
    "    df['ET_ERA5L'] = et_df\n",
    "    \n",
    "    # Load precip from RÁV-II\n",
    "    rav_precip = rav_data[gauge_id]\n",
    "    rav_precip_df = pds.DataFrame(rav_precip)\n",
    "    rav_precip_df.columns=['P_rav']\n",
    "    df['P_rav'] = rav_precip_df\n",
    "    \n",
    "    # Load ref_ET from RÁV-II \n",
    "    rav_PET = PET_PM_ravII_grdflx[gauge_id]\n",
    "    rav_PET_df = pds.DataFrame(rav_PET)\n",
    "    rav_PET_df.columns=['PET_rav']\n",
    "    df['PET_rav'] = rav_PET_df\n",
    "    \n",
    "    # Load ref_ET from RÁV-II\n",
    "    rav_ET = rav_ET_sum[gauge_id]\n",
    "    rav_ET_df = pds.DataFrame(rav_ET)\n",
    "    rav_ET_df.columns=['ET_rav']\n",
    "    df['ET_rav'] = rav_ET_df\n",
    "    \n",
    "    # Add column 'water_year'\n",
    "    water_years = [(d - dt.timedelta(days=273)).year for d in df.index]\n",
    "    df['water_year'] = water_years\n",
    "\n",
    "    # Find the index of years where we have data coverage over the threshhold value (90%)\n",
    "    meas_dropna = df.dropna()\n",
    "    water_years_dropna = [(d - dt.timedelta(days=273)).year for d in meas_dropna.index]\n",
    "    meas_dropna.loc[:,'water_year'] = water_years_dropna\n",
    "    coverage_water_years = meas_dropna['Value'].groupby(water_years_dropna).count()/365\n",
    "\n",
    "    valid = coverage_water_years.loc[coverage_water_years>=thresh]\n",
    "    yearcount = len(valid)\n",
    "\n",
    "    # Get all data for the water years that are valid\n",
    "    df_valid = df[df['water_year'].isin(valid.index)]\n",
    "    df_valid = df_valid.copy()\n",
    "\n",
    "    # Convert streamflow measurements from daily mean m3/s to mm/d\n",
    "    streamflow_values = df_valid['Value'].values\n",
    "    streamflow_values_mmday = 1000*(streamflow_values*86400/(catchment_attrs.loc[gauges[gauges['V_no']==gauge].index]['area_calc'].values[0]*1000000))\n",
    "\n",
    "    df_valid.loc[:,'Q'] = streamflow_values_mmday\n",
    "    \n",
    "    if yearcount>=3:\n",
    "        data_for_valid_years[gauge] = df_valid[['Q','Quality','P_ERA5L','PET_ERA5L','ET_ERA5L','water_year','P_rav','PET_rav','ET_rav']]\n",
    "    #else:\n",
    "        #print('Only %s valid years for %s' % (yearcount,gauge))\n",
    "        # We can plot the data to double check\n",
    "#         meas_dict[gauge].plot()\n",
    "#         plt.title('%s, %s' %(gauge,gauges[gauges['V_no']==gauge]['river'].values[0]))\n",
    "#         plt.show()\n",
    "    valid_years_lengths[gauge_id] = {'year_count': yearcount}\n",
    "    \n",
    "valid_years_df = pds.DataFrame(valid_years_lengths)\n",
    "valid_years_df = valid_years_df.reindex(sorted(valid_years_df.columns), axis=1)\n",
    "valid_years_df = valid_years_df.T\n",
    "\n",
    "# Save wbalance df\n",
    "w_balance_dict = dict()\n",
    "for gauge in data_for_valid_years.keys():\n",
    "    # Fetch the gauge ID from the station name\n",
    "    gauge_id = catchment_attrs.loc[gauges[gauges['V_no']==gauge].index].index.values[0]\n",
    "    \n",
    "    # Create a dataframe with the measured discharge, quality value, precipitation and water year\n",
    "    df = data_for_valid_years[gauge]\n",
    "    \n",
    "    # Calculate means\n",
    "    w_balance_dict[gauge_id] = df.mean()\n",
    "    \n",
    "wb_df = pds.DataFrame(w_balance_dict)\n",
    "wb_df = wb_df.reindex(sorted(wb_df.columns), axis=1)\n",
    "final_wb_df = wb_df.T\n",
    "\n",
    "\n",
    "# Save water balance dataframe as .csv\n",
    "folder = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\lamah_ice\\A_basins_total_upstrm\\1_attributes\\final_attributes\")\n",
    "path = folder/'water_balance.csv'\n",
    "final_wb_df.index.name = 'id'\n",
    "final_wb_df[['Q','P_ERA5L','PET_ERA5L','ET_ERA5L','water_year','P_rav','PET_rav','ET_rav']].to_csv(path,sep=';')\n",
    "\n",
    "# Now the hydrological signatures calculations:\n",
    "signs_dict = dict()\n",
    "for gauge in data_for_valid_years.keys():\n",
    "    # Fetch the gauge ID from the station name\n",
    "    gauge_id = gauges[gauges['V_no']==gauge].index[0] \n",
    "        \n",
    "    # Create a dataframe with the measured discharge, quality value, precipitation and water year\n",
    "    df = data_for_valid_years[gauge]\n",
    "    \n",
    "    # Calculate signatures\n",
    "    try:\n",
    "        signs_dict[gauge_id] = hydroanalysis.utils.calculate_multiple_signatures([hydroanalysis.streamflow_signatures.calculate_q_mean,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_runoff_ratio,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_stream_elas,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_slope_fdc,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_baseflow_index,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_hfd_mean,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_q_5,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_q_95,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_high_q_freq_dur,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_low_q_freq_dur,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_zero_q_freq],\n",
    "                                                      df['Q'].values, #streamflow_values_mmday,\n",
    "                                                      df['Quality'].values,\n",
    "                                                      df['P_rav'].values,\n",
    "                                                      df['water_year'].values) #np.array(water_years))\n",
    "    except TypeError:\n",
    "        print('Wrong output type for %s' % gauge) \n",
    "\n",
    "signs_df = pds.DataFrame(signs_dict)\n",
    "signs_df = signs_df.reindex(sorted(signs_df.columns), axis=1)\n",
    "final_df = signs_df.T\n",
    "\n",
    "# Rename the columns \n",
    "final_df = final_df[['calculate_q_mean', 'calculate_runoff_ratio',\n",
    "       'calculate_stream_elas_Sankarasubramanian',\n",
    "       'calculate_slope_fdc_Addor',\n",
    "       'calculate_baseflow_index', 'calculate_hfd_mean_hfd_mean',\n",
    "       'calculate_q_5', 'calculate_q_95',\n",
    "       'calculate_high_q_freq_dur_hq_freq', 'calculate_high_q_freq_dur_hq_dur',\n",
    "       'calculate_low_q_freq_dur_lq_freq', 'calculate_low_q_freq_dur_lq_dur',\n",
    "       'calculate_zero_q_freq']]\n",
    "new_column_names = ['q_mean','runoff_ratio','stream_elas',\n",
    "                    'slope_fdc','baseflow_index_ladson',\n",
    "                    'hfd_mean','Q5',\n",
    "                    'Q95','high_q_freq','high_q_dur','low_q_freq',\n",
    "                    'low_q_dur','zero_q_freq']\n",
    "final_df.columns=new_column_names\n",
    "\n",
    "path = folder/'valid_years_10_perc_tol_1981_2018_ravII.csv'\n",
    "valid_years_df.index.name = 'id'\n",
    "valid_years_df.to_csv(path,sep=';')\n",
    "\n",
    "path = folder/'hydro_indices.csv'\n",
    "final_df.index.name = 'id'\n",
    "final_df.to_csv(path,sep=';')\n",
    "print('Mean valid years: %s' %valid_years_df[valid_years_df>2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6cde4825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "466f0b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_years_df[valid_years_df>2].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-digit",
   "metadata": {},
   "source": [
    "# Unfiltered observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbfbff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 0 valid years for V100\n",
      "Only 0 valid years for V100_2\n",
      "Only 0 valid years for V597\n",
      "Only 2 valid years for 154259\n",
      "Only 2 valid years for V515\n",
      "Only 0 valid years for V125\n",
      "Only 0 valid years for V94\n",
      "Only 0 valid years for V95\n",
      "Only 0 valid years for V98\n",
      "Only 2 valid years for V280\n",
      "Only 0 valid years for V99\n",
      "Only 0 valid years for V252\n",
      "Only 0 valid years for V132\n",
      "Only 2 valid years for V618\n",
      "Mean valid years: year_count    22.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Same code as above. We now use the splitted_gauge_dict that includes all streamflow measurements (unfiltered version) \n",
    "\n",
    "# Create a dictionary with measurements that are ready for processing by the hydroanalysis package:\n",
    "meas_dict = dict()\n",
    "for gauge in splitted_gauge_dict.keys():    \n",
    "    df = splitted_gauge_dict[gauge].copy()\n",
    "    df.columns=['Value','Quality']\n",
    "    \n",
    "    df.loc[df['Quality']<=200, 'Quality']=0\n",
    "    df.loc[df['Quality']>200, 'Quality']=1\n",
    "    # Rename the splitted gauge names (V112_1, V100_1) to their proper gauge names (V112,V100)\n",
    "    if gauge=='V112_1':\n",
    "        gauge='V112'\n",
    "    elif gauge=='V68_1':\n",
    "        gauge='V68'\n",
    "    elif gauge=='V100_1':\n",
    "        gauge='V100'\n",
    "    meas_dict[gauge] = df\n",
    "\n",
    "# Create a dictionary that contains a dataframe for each gauge\n",
    "# The dataframe has the columns: \n",
    "# Streamflow meas, quality code, precip from ERA5-Land and RAV-II, PET, total evaporation from ERA5-Land and water year\n",
    "# If we have less than 3 years of valid data, we don't include the gauge\n",
    "\n",
    "data_for_valid_years = dict()\n",
    "\n",
    "# This dictionary keeps track of how many valid years we have for each gauge\n",
    "valid_years_lengths = dict()\n",
    "\n",
    "thresh = 0.9\n",
    "\n",
    "for gauge in meas_dict.keys():\n",
    "    # Fetch the gauge ID from the station name   \n",
    "    gauge_id = gauges[gauges['V_no']==gauge].index[0]\n",
    "    # Load the discharge values\n",
    "    df = meas_dict[gauge].copy()\n",
    "    try:\n",
    "        df.index= df.index.date\n",
    "    except:\n",
    "        print('df index is already on date format for %s' %gauge)\n",
    "\n",
    "    # Load ERA precip\n",
    "    wshed_precip = precip[gauge_id]\n",
    "    precip_df = pds.DataFrame(wshed_precip)\n",
    "    precip_df.columns=['P_ERA5L']\n",
    "    df['P_ERA5L'] = precip_df\n",
    "    \n",
    "    # Load PET\n",
    "    wshed_pet = pet[gauge_id]\n",
    "    pet_df = pds.DataFrame(wshed_pet)\n",
    "    pet_df.columns=['PET_ERA5L']\n",
    "    df['PET_ERA5L'] = pet_df\n",
    "    \n",
    "    # Load total evaporation\n",
    "    wshed_evap = evap[gauge_id]\n",
    "    et_df = pds.DataFrame(wshed_evap)\n",
    "    et_df.columns=['ET_ERA5L']\n",
    "    df['ET_ERA5L'] = et_df\n",
    "    \n",
    "    # Load precip from RÁV-II\n",
    "    rav_precip = rav_data[gauge_id]\n",
    "    rav_precip_df = pds.DataFrame(rav_precip)\n",
    "    rav_precip_df.columns=['P_rav']\n",
    "    df['P_rav'] = rav_precip_df\n",
    "    \n",
    "    # Load ref_ET from RÁV-II \n",
    "    rav_PET = PET_PM_ravII_grdflx[gauge_id]\n",
    "    rav_PET_df = pds.DataFrame(rav_PET)\n",
    "    rav_PET_df.columns=['PET_rav']\n",
    "    df['PET_rav'] = rav_PET_df\n",
    "    \n",
    "    # Load ET from RÁV-II\n",
    "    rav_ET = rav_ET_sum[gauge_id]\n",
    "    rav_ET_df = pds.DataFrame(rav_ET)\n",
    "    rav_ET_df.columns=['ET_rav']\n",
    "    df['ET_rav'] = rav_ET_df\n",
    "    \n",
    "    # Add column 'water_year'\n",
    "    water_years = [(d - dt.timedelta(days=273)).year for d in df.index]\n",
    "    df['water_year'] = water_years\n",
    "\n",
    "    # Find the index of years where we have data coverage over the threshhold value (90%)\n",
    "    meas_dropna = df.dropna()\n",
    "    water_years_dropna = [(d - dt.timedelta(days=273)).year for d in meas_dropna.index]\n",
    "    meas_dropna.loc[:,'water_year'] = water_years_dropna\n",
    "    coverage_water_years = meas_dropna['Value'].groupby(water_years_dropna).count()/365\n",
    "\n",
    "    valid = coverage_water_years.loc[coverage_water_years>=thresh]\n",
    "    yearcount = len(valid)\n",
    "\n",
    "    # Get all data for the water years that are valid\n",
    "    df_valid = df[df['water_year'].isin(valid.index)]\n",
    "    df_valid = df_valid.copy()\n",
    "\n",
    "    # Convert streamflow measurements from daily mean m3/s to mm/d\n",
    "    streamflow_values = df_valid['Value'].values\n",
    "    streamflow_values_mmday = 1000*(streamflow_values*86400/(catchment_attrs.loc[gauges[gauges['V_no']==gauge].index]['area_calc'].values[0]*1000000))\n",
    "\n",
    "    df_valid.loc[:,'Q'] = streamflow_values_mmday\n",
    "    \n",
    "    if yearcount>=3:\n",
    "        data_for_valid_years[gauge] = df_valid[['Q','Quality','P_ERA5L','PET_ERA5L','ET_ERA5L','water_year','P_rav','PET_rav','ET_rav']]\n",
    "    else:\n",
    "        print('Only %s valid years for %s' % (yearcount,gauge))\n",
    "#         meas_dict[gauge].plot()\n",
    "#         plt.title('%s, %s' %(gauge,gauges[gauges['V_no']==gauge]['river'].values[0]))\n",
    "#         plt.show()\n",
    "    valid_years_lengths[gauge_id] = {'year_count': yearcount}\n",
    "    \n",
    "valid_years_df = pds.DataFrame(valid_years_lengths)\n",
    "valid_years_df = valid_years_df.reindex(sorted(valid_years_df.columns), axis=1)\n",
    "valid_years_df = valid_years_df.T\n",
    "\n",
    "# Save wbalance df\n",
    "w_balance_dict = dict()\n",
    "for gauge in data_for_valid_years.keys():\n",
    "    # Fetch the gauge ID from the station name\n",
    "    gauge_id = catchment_attrs.loc[gauges[gauges['V_no']==gauge].index].index.values[0]\n",
    "    \n",
    "    # Create a dataframe with the measured discharge, quality value, precipitation and water year\n",
    "    df = data_for_valid_years[gauge]\n",
    "    \n",
    "    # Calculate means\n",
    "    w_balance_dict[gauge_id] = df.mean()\n",
    "    \n",
    "wb_df = pds.DataFrame(w_balance_dict)\n",
    "wb_df = wb_df.reindex(sorted(wb_df.columns), axis=1)\n",
    "final_wb_df = wb_df.T\n",
    "\n",
    "# Save water balance dataframe as .csv\n",
    "folder = Path(r\"C:\\Users\\hordurbhe\\Dropbox\\UW\\lamah_ice\\lamah_ice\\A_basins_total_upstrm\\1_attributes\\final_attributes\")\n",
    "path = folder/'water_balance_unfiltered.csv'\n",
    "final_wb_df.index.name = 'id'\n",
    "final_wb_df[['Q','P_ERA5L','PET_ERA5L','ET_ERA5L','water_year','P_rav','PET_rav','ET_rav']].to_csv(path,sep=';')\n",
    "\n",
    "# Now the hydrological signatures calculations:\n",
    "signs_dict = dict()\n",
    "for gauge in data_for_valid_years.keys():\n",
    "    # Fetch the gauge ID from the station name\n",
    "    gauge_id = gauges[gauges['V_no']==gauge].index[0] \n",
    "    \n",
    "    # Create a dataframe with the measured discharge, quality value, precipitation and water year\n",
    "    df = data_for_valid_years[gauge]\n",
    "    \n",
    "    # Calculate signatures\n",
    "    try:\n",
    "        signs_dict[gauge_id] = hydroanalysis.utils.calculate_multiple_signatures([hydroanalysis.streamflow_signatures.calculate_q_mean,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_runoff_ratio,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_stream_elas,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_slope_fdc,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_baseflow_index,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_hfd_mean,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_q_5,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_q_95,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_high_q_freq_dur,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_low_q_freq_dur,\n",
    "                                                       hydroanalysis.streamflow_signatures.calculate_zero_q_freq],\n",
    "                                                      df['Q'].values, #streamflow_values_mmday,\n",
    "                                                      df['Quality'].values,\n",
    "                                                      df['P_rav'].values,\n",
    "                                                      df['water_year'].values) #np.array(water_years))\n",
    "    except TypeError:\n",
    "        print('Wrong output type for %s' % gauge) \n",
    "\n",
    "signs_df = pds.DataFrame(signs_dict)\n",
    "signs_df = signs_df.reindex(sorted(signs_df.columns), axis=1)\n",
    "final_df = signs_df.T\n",
    "\n",
    "# Rename the columns \n",
    "final_df = final_df[['calculate_q_mean', 'calculate_runoff_ratio',\n",
    "       'calculate_stream_elas_Sankarasubramanian',\n",
    "       'calculate_slope_fdc_Addor',\n",
    "       'calculate_baseflow_index', 'calculate_hfd_mean_hfd_mean',\n",
    "       'calculate_q_5', 'calculate_q_95',\n",
    "       'calculate_high_q_freq_dur_hq_freq', 'calculate_high_q_freq_dur_hq_dur',\n",
    "       'calculate_low_q_freq_dur_lq_freq', 'calculate_low_q_freq_dur_lq_dur',\n",
    "       'calculate_zero_q_freq']]\n",
    "new_column_names = ['q_mean','runoff_ratio','stream_elas',\n",
    "                    'slope_fdc','baseflow_index_ladson',\n",
    "                    'hfd_mean','Q5',\n",
    "                    'Q95','high_q_freq','high_q_dur','low_q_freq',\n",
    "                    'low_q_dur','zero_q_freq']\n",
    "final_df.columns=new_column_names\n",
    "\n",
    "path = folder/'valid_years_10_perc_tol_1981_2018_ravII_unfiltered.csv'\n",
    "valid_years_df.index.name = 'id'\n",
    "valid_years_df.to_csv(path,sep=';')\n",
    "\n",
    "path = folder/'hydro_indices_unfiltered.csv'\n",
    "final_df.index.name = 'id'\n",
    "final_df.to_csv(path,sep=';')\n",
    "print('Mean valid years: %s' %valid_years_df[valid_years_df>2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98718574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason that we have more gauges than reported in the gauge data availability analysis in the paper (97 vs. 94)\n",
    "# Is that here we are using the splitted gauges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1111c4",
   "metadata": {},
   "source": [
    "# Calculate a \"water balance\" .csv file with only meteorological time series means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this version, no years are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6aa6ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '1981-10-01'\n",
    "end = '2018-09-30'\n",
    "merged_df = pds.concat([\n",
    "    precip[start:end].mean().sort_index().rename('P_ERA5L_%s' % start[:4]),\n",
    "    evap[start:end].mean().sort_index().rename('ET_ERA5L_%s' % start[:4]),\n",
    "    pet[start:end].mean().sort_index().rename('PET_ERA5L_%s' % start[:4]),\n",
    "    rav_data[start:end].mean().sort_index().rename('P_ravII_%s' % start[:4]),\n",
    "    rav_ET_sum[start:end].mean().sort_index().rename('ET_ravII_%s' % start[:4]),\n",
    "    PET_PM_ravII_grdflx[start:end].mean().sort_index().rename('PET_ravII_%s' % start[:4]),\n",
    "], axis=1)\n",
    "merged_df = merged_df.loc[merged_df.index[:-7]]\n",
    "savepath = Path('C:\\\\Users\\\\hordurbhe\\\\Documents\\\\Vinna\\\\lamah\\\\lamah_ice\\\\lamah_ice\\\\A_basins_total_upstrm\\\\1_attributes\\\\meteorological_data_means_1981_to_2018.csv')\n",
    "merged_df.to_csv(savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4ff52",
   "metadata": {},
   "source": [
    "# Now we export carra too for 1991-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6914ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '1991-10-01'\n",
    "end = '2018-09-30'\n",
    "merged_df = pds.concat([\n",
    "    precip[start:end].mean().sort_index().rename('P_ERA5L_%s' % start[:4]),\n",
    "    evap[start:end].mean().sort_index().rename('ET_ERA5L_%s' % start[:4]),\n",
    "    pet[start:end].mean().sort_index().rename('PET_ERA5L_%s' % start[:4]),\n",
    "    rav_data[start:end].mean().sort_index().rename('P_ravII_%s' % start[:4]),\n",
    "    rav_ET_sum[start:end].mean().sort_index().rename('ET_ravII_%s' % start[:4]),\n",
    "    PET_PM_ravII_grdflx[start:end].mean().sort_index().rename('PET_ravII_%s' % start[:4]),\n",
    "    combined_carra[start:end].mean().sort_index().rename('P_carra_%s' % start[:4]),\n",
    "\n",
    "], axis=1)\n",
    "merged_df = merged_df.loc[merged_df.index[:-7]]\n",
    "savepath = Path(r'C:\\Users\\hordurbhe\\Documents\\Vinna\\lamah\\lamah_ice\\lamah_ice\\A_basins_total_upstrm\\1_attributes\\meteorological_data_means_1991_to_2018.csv')\n",
    "merged_df.to_csv(savepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kiwis]",
   "language": "python",
   "name": "conda-env-kiwis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
